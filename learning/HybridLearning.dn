const char DEFAULT_SERVER_IP[] = "127.0.0.1"
const int DEFAULT_SERVER_PORT = 8008

//at start of operation no operating env is know, so a random
//default is given
const char key[] = "DEFAULT_ENV"

const int OBSERVATION_WINDOW = 1000 //ms

const dec e = 2.71828

component provides App requires data.adt.HashTable, io.Output out, pal.control.RestAPI, net.http.HTTPRequest request, ml.rl.RL, time.Timer timer, data.IntUtil iu, data.DecUtil du, data.json.JSONParser parser, ml.cluster.Clustering, util.Random, util.Math maff {

	dec normalisation_low = 0.0
	dec normalisation_high = 100.0

    HashTable detectedEnvs = new HashTable()

    //set params for clustering algorithm (dataPoints + k decided by RL)
    int dimentions = 3 //events, responsetime, range in first two since t = 0
    int k = 1

    String actions[] = new String[3]
    actions[0] = new String("new env")
    actions[1] = new String("delete env")
    //actions[2] = new String("reduce range")
    //actions[3] = new String("increase range")
    actions[2] = new String("do nothing")

    //give the learner some memory of previous server state
    // index 0 is lower bound
    // index 1 is upper bound
    dec dim1[] = new dec[2]
    dim1[0] = 0.0
    dim1[1] = 0.0
    dec dim2[] = new dec[2]
    dim2[0] = 0
    dim2[1] = 0

    Cluster previousClusters[]

    dec positive(dec d) {
        if (d < 0.0) d = d*-1.0
        return d
    }
    
    void updateDataPoints(Clustering cluster, dec envModel[]) {
        cluster.addValue(envModel, null)
        return
    }

    Cluster[] takeAction(Clustering cluster, int action) {
        if (action == 0) {
            //create new env
            k += 1
        }
        else if (action == 1) {
            if (k == 1) {}
            else {
                k -= 1
            }
        }
        else if(action == 2) {
            //do nothing
        }
        else {
            out.print("error: invalid action\n")
            return null
        }
        //recluster
        return cluster.cluster(k)
    }

    dec distanceBetweenPointsNDim(dec a[], dec b[], int dim) {
        dec distance = 0
        for (int i = 0; i < dimentions; i++) {
            distance += positive(maff.power((a[i] - b[i]), 2))
        }
        return distance
    }

    dec evaluateCluster(Cluster clusters[]) {
        dec penalty = 0.0
        dec reward = 0.0
        //is k proportional to the delta in range between env[2] and prevEnv[2]??
        //penalise for increasing k
        if (clusters.arrayLength > previousClusters.arrayLength) {
            penalty += 0.9
        }
        //reward for having k-means further, weighted towards greater k
        if (clusters.arrayLength > 1) {
            dec avgDistanceBetweenClusters = 0.0
            dec sumDistanceBetweenClusters = 0.0

            dec countOfDistances =  dimentions*(clusters.arrayLength*(clusters.arrayLength-1))/2 //full mesh graph

            int m = 0
            for (int i = 0; i < clusters.arrayLength; i++) {
                for (int j = 0; j < clusters.arrayLength-m; j++) {
                    for (int n = 0; n < clusters[i].mean.arrayLength; n++) {
                        sumDistanceBetweenClusters += distanceBetweenPointsNDim(clusters[i].mean, clusters[j].mean, dimentions)
                    }
                }
                m++
            }

            avgDistanceBetweenClusters = sumDistanceBetweenClusters / countOfDistances

            reward = clusters.arrayLength*avgDistanceBetweenClusters
            dec x = reward
            reward = (maff.power(e, x))/(maff.power(e, x)+1)
            reward = positive(reward)
            out.print("weighted reward: \n")
            out.print(du.makeString(reward))
            out.print("\n")
        }
        reward = reward - penalty
        out.print(du.makeString(reward))
        out.print("\n")
        if (reward < 0.0) reward = 0.0
        return reward
    }

    dec evaluateClusterSimple(int action) {
        out.print(iu.makeString(detectedEnvs.getLength()))
        out.print("\n")
        out.print(iu.makeString(k))
        out.print("\n")
        if (k == detectedEnvs.getLength()) {
            //if (actions[action] == "do nothing")
            if (action == 2)
            {
                out.print("Rewarded 2\n")
                return 2.0
            }
            else {
                out.print("Rewarded 1\n")
                return 1.0
            }
        }
        else {
            out.print("Rewarded 0\n")
            return 0.0
        }
    }
    
    int App:main(AppParam params[]) {
		//connect to rest API of PAL and set config to index 0 (or any other index you like...)
		RestAPI restAPI = new RestAPI(DEFAULT_SERVER_IP, DEFAULT_SERVER_PORT)
		
		RL onlineLearning = new RL()
		onlineLearning.setExplorationPenalty(1.0)
		onlineLearning.setActions(actions)

        Clustering cluster = new Clustering(dimentions)

        while (true) {
            //consult pal for server state
			HTTPResponse r = request.get("http://$DEFAULT_SERVER_IP:$DEFAULT_SERVER_PORT/meta/get_perception", null)
			
			//extract the env metrics
			JSONElement doc = parser.parseDocument(r.content)
			JSONElement se = null
			
			//NOTE: if your observation window is "too small" you may get zero reward - e.g. for a web server, if no requests were served in that time
		    
            dec envModel[] = new dec[dimentions]
			if ((se = parser.getValue(doc, "metrics")) != null)
				{
                    dec vk = iu.intFromString(parser.getValue(se.children[0], "value").value)
                    dec vt = iu.intFromString(parser.getValue(se.children[0], "count").value)
                    
                    envModel[0] = vk / vt
                    if (envModel[0] > dim1[1]) {
                        dim1[1] = envModel[0]
                    }
                    out.print("\n")
                    else if (envModel[0] < dim1[0]) {
                        dim1[0] = envModel[0]
                    }
				
				}
			
			// "environment" data is acquired as below from the JSON data, if you want it...
			if ((se = parser.getValue(doc, "events")) != null)
				{
				for (int i = 0; i < se.children.arrayLength; i ++)
					{
                        char name[] = parser.getValue(se.children[i], "name").value

                        dec vk = iu.intFromString(parser.getValue(se.children[i], "value").value)
                        dec vt = iu.intFromString(parser.getValue(se.children[i], "count").value)
                        
                        envModel[1] = vk / vt
                        if (detectedEnvs.get(du.makeString(envModel[1])) == null) {
                            out.print("event not already encountered\n")
                            detectedEnvs.put(du.makeString(envModel[1]), new Data())
                        }
                        if (envModel[1] > dim2[1]) {
                            dim2[1] = envModel[1]
                        }
                        else if (envModel[1] < dim2[0]) {
                            dim2[0] = envModel[1]
                        }
                        out.print("envModel.1:\n")
                        out.print(du.makeString(envModel[1]))
                        out.print("\n")
					}
				}
            //update range since t=0
            envModel[2] = ((dim1[1]-dim1[0]) + (dim2[1] - dim2[0]))/2
            //add server state to set of data points for future clustering
		    updateDataPoints(cluster, envModel)	

            //get rl's next Action
            int ndx = onlineLearning.getAction()

            out.print("Action: $ndx\n")
            out.print("Discrete Enviroment Count: $k\n")

            if (k == 1 && ndx == 1) {
                //this action will give zero clusters
                //wait
                timer.sleep(OBSERVATION_WINDOW)

                //give zero reward
                onlineLearning.setReward(0.0)
            }
            else {
                //recluster
                Cluster clusters[] = takeAction(cluster, ndx)

                //wait
                timer.sleep(OBSERVATION_WINDOW)

                //evaluate the clusters
                dec reward = evaluateClusterSimple(ndx)
                previousClusters = clusters

                //give reward
                onlineLearning.setReward(reward)
            }
        }

        return 0
    }
}
